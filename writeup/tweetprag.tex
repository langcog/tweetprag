\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2015}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code to use with NAACL/ACL style files to simulate natbib's 
% \citealt, which prints citations with no parentheses. This should
% work if pasted into the preamble. \cite, \newcite, and \shortcite
% should continue to work as before.

\makeatletter

\def\citealt{\def\citename##1{{\frenchspacing##1} }\@internalcitec}

\def\@citexc[#1]#2{\if@filesw\immediate\write\@auxout{\string\citation{#2}}\fi
  \def\@citea{}\@citealt{\@for\@citeb:=#2\do
    {\@citea\def\@citea{;\penalty\@m\ }\@ifundefined
       {b@\@citeb}{{\bf ?}\@warning
       {Citation `\@citeb' on page \thepage \space undefined}}%
{\csname b@\@citeb\endcsname}}}{#1}}

\def\@internalcitec{\@ifnextchar [{\@tempswatrue\@citexc}{\@tempswafalse\@citexc[]}}

\def\@citealt#1#2{{#1\if@tempswa, #2\fi}}

\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Effects of non-linguistic context
\title{Shared common ground influences information density in microblog texts\Thanks{Thanks to...}}
% common ground decreases information

\author{Gabriel Doyle\\
	    Stanford University\\
	    450 Serra Mall\\
	    Stanford, CA 94305, USA\\
	    {\tt gdoyle@stanford.edu}
	  \And
          Michael C. Frank\\
	    Stanford University\\
	    450 Serra Mall\\
	    Stanford, CA 94305, USA\\
	    {\tt mcfrank@stanford.edu}}

\date{}


\begin{document}
\maketitle
\begin{abstract}
Natural languages offer many possible ways to say the same thing. If speakers use language rationally, they should structure their messages to achieve approximately uniform information density (UID), in order to maximize transmission via a noisy channel. Previous work identified a consistent increase in linguistic information across sentences in text as a signature of the UID hypothesis. This increase was derived on the basis of a predicted increase in shared non-linguistic common ground, but common ground was never measured. We test this prediction for the first time using microblog texts from twitter. We take advantage of tweets about a single shared event (the baseball World Series) to measure common ground, identifying both gradual and rapid changes in information content in response to in-game events. These findings lend further support to the UID hypothesis and underscore the importance of non-linguistic common ground for language understanding.
\end{abstract}

\section{Introduction}

Natural language is an incredibly powerful method for representing and communicating information. One consequence of this power is that there is an infinity of different ways to structure any particular message. How do speakers choose between different possible structures? One provocative hypothesis is that they attempt to structure their message to best convey their intended meaning in the context of their communication. On this view, the use of natural languages is assumed to follow optimal information transmission results from information theory \cite{shannon1948}. In particular, speakers should structure their messages to approximate \emph{uniform information density} across symbols (words and phonemes), which is optimal for transmission of information through a noisy channel. 

At least three lines of evidence suggest that speakers do make choices to increase the uniformity of information density across their utterances. First, speakers phonologically reduce more predictable material \cite{aylett2004,aylett2006}. Second, they omit or reduce optional lexical material in cases where the subsequent syntactic information is relatively more predictable \cite{levy2007,frank2008,jaeger2010}. Third, and most relevant to our current hypothesis, speakers appear to increase the complexity of their utterances as a discourse develops context \cite{genzel2002,genzel2003,qian2012}. This last is most relevant to our current investigation so we expand on it in more detail.

Following the UID hypothesis, \citealt{genzel2002} proposed that $H(Y_i)$, the total entropy of part $i$ of a message (e.g., a word), is constant. They compute this expression by considering $X_i$, the random variable representing the precise word that will appear at position $i$, conditioned on all the previous observed words. They then further factor this expression into two terms:

\begin{eqnarray}
H(Y_i) &=& H(X_i | C_i, L_i) \\
&=& H(X_i | L_i) - I(X_i ; C_i | L_i) \label{eq:gc}
\end{eqnarray}

\noindent where the first term $H(X_i | L_i)$ is the dependence of the current word on only the local linguistic context (e.g. within the rest of the sentence $L_i$) and the second is the mutual information between the current word and the broader linguistic context $C_i$, given the rest of the current sentence. On their logic, with greater amounts of contextual information, the predictability of linguistic material based on context, $I(X_i | C_i, L_i)$ must go up. Therefore, they predicted that $H(X_i|L_i)$ should also increase, so as to result in a constant total amount of information. For convenience, we rewrite Equation \ref{eq:gc} as

\begin{equation}
\label{eq:lambda}
H(X_i | L_i) - I(X_i ; C_i | L_i) = \lambda
\end{equation}

\noindent where $\lambda$ is a constant. 

Genzel and Charniak then approximated $H(X_i | L_i)$ using a number of methods and showed that it did increase systematically in documents. Later work showed that this increase was strongest within paragraphs and was general across document types \cite{genzel2003} and languages \cite{qian2012}. No attempt was made in this work, however, to measure shared context (and its influence on message expectations) directly. This challenge is the focus of our current work. 

\subsection{Contextual effects on complexity}
In the previous work, the context is built up from the preceding linguistic information without any information being introduced non-linguistically.  In real-world events, though, the world itself is changing providing new non-linguistic information.  When there is both linguistic and non-linguistic information, both information streams are passing through the noisy channel, so the relevant quantity is not the marginal entropy of the linguistic stream, as in previous work, but the joint entropy of both streams.  Let $T_j$ be the linguistic information (the tweets) in part $j$ of the discourse, and $E_j$ be the non-linguistic information (the events) in part $j$.  If $C_j$ is the built-up context from the preceding parts $\{1,\cdots,j-1\}$ of the discourse, then we can break down the joint entropy as:

\begin{align}
\nonumber H(T_j,E_j|C_{j-1}) & = H(T_j|E_j,C_{j-1}) + H(E_j|C_{j-1}) \\
\nonumber                    & = H(T_j|C_{j-1}) - I(T_j;E_j|C_{j-1}) + H(E_j|C_{j-1}) \\
\nonumber                    & = H(T_j) - I(T_j;C_{j-1}) - I(T_j;E_j|C_{j-1}) + H(E_j|C_{j-1}) \\
\label{eq:joint-entropy}     & = H(T_j) - I(T_j;E_j,C_{j-1}) + H(E_j|C_{j-1})
\end{align}

By the UID hypothesis, we expect the left-hand side of this equation to be constant, as it is the information content of each part of the discourse.  The first term of the right-hand side is the out-of-context entropy of the linguistic information. The second term is the mutual information of the linguistic information and the union of the preceding context plus the current non-linguistic information (the events occurring at the time).  The third term is the entropy of the non-linguistic information, given the preceding context.

This breakdown suggests that rational participants in a discourse will exhibit both slow and fast adaptation to context in order to maintain overall constant entropy.  As context slowly builds, the mutual information term grows (and the non-linguistic entropy likely shrinks), resulting in the time-based increase in $H(T_j)$ that previous work has found.  In addition, an individual event can have high or low information content given the context, without having a large effect on the mutual information term.  To maintain constant entropy, high-information events should be accompanied by low-information linguistic responses, and vice versa.  With an operationalization of shared context, we should be able to observe these two types of adaptation directly, not just via the increasing trend shown in previous work \cite{genzel2002,qian2012}.  

In psycholinguistics, the notion of shared \emph{common ground} is a more precise replacement for the general notion of ``context'' \cite{clark1996}. Common ground is defined as the knowledge that participants in a discourse have and that participants know other participants have. A broad literature suggests that speakers consider common ground in selecting the appropriate expression to refer to a particular object \cite{brennan1996,metzing2003}. In principle, Genzel and Charniak's formulation can be considered as capturing the relationship between shared common ground and the predictability of language. 

To test this prediction, we leverage twitter, a popular microblogging service, to operationalize common ground. Because of its structure, twitter is an ideal platform for this investigation. One common method of using twitter is to mark messages with hashtags, which serve as ad-hoc categories, allowing anyone interested in a topic to find the messages relevant to that topic. This strategy is especially used when users are commenting on an external event (e.g. a sporting, media, or political event). We focus here on the World Series of baseball, an annual sporting event with large viewership and a single broadcast stream; in this case, the hashtag is \texttt{\#worldseries}. Hashtagged messages are part of a discourse with extremely limited prior linguistic context, as no two tweeters will have seen the same set of tweets. The total shared context with the audience that can be assumed by the writer of a tweet is the non-linguistic content of the event being hashtagged. 

We begin by describing our corpus and our method of calculating linguistic content (by computing entropy within a simple $n$-gram model). We then investigate gradual changes in word-by-word information content as the event goes on (testing adaptation driven by contextual mutual information in Equation \ref{eq:joint-entropy}, replicating \citealt{genzel2002}) and rapid changes in the total information content of tweets in response to important in-game events (testing adaptation driven by non-linguistic information in Equation \ref{eq:joint-entropy}). We end by considering a number of control analyses that provide evidence against alternative accounts of our results. 

%\subsection{First version of contextual effects}
%
%Although Genzel and Charniak stated the regularity in terms of the current word, an alternative formulation would be in terms of the total information content of the utterance. We can see this by summing Equation \ref{eq:lambda} across words:
%
%\begin{equation}
%H(X) - I(X | C) = |X| \lambda
%\end{equation}
%
%\noindent where $H(X) = \sum_i{H(X_i|L_i)}$ and $I(X|C) = \sum_i{I(X_i | C_i, L_i)}$. Hence,
%
%\begin{equation}
%\label{eq:prop}
%H(X) \propto -I(X|C).
%\end{equation}
%
%\noindent This proportionality indicates that as the amount of shared context goes up, the total linguistic complexity should go down. Thus, if we have an operationalization of shared context, we should be able to observe this proportionality directly, not just via the increasing trend shown in previous work \cite{genzel2002,qian2012}. 
%
%In psycholinguistics, the notion of shared \emph{common ground} is a more precise replacement for the general notion of ``context'' \cite{clark1996}. Common ground defined as constituted the total of knowledge that communicators share with their audience. In particular, a broad literature suggests that speakers consider common ground in selecting the appropriate expression to refer to a particular object \cite{brennan1996,metzing2003}. In principle, Genzel and Charniak's formulation can be considered as capturing the relationship between shared common ground and the predictability of language. 
%
%To test this prediction, we leverage twitter, a popular microblogging service, to operationalize common ground. Because of its structure, twitter is an ideal platform for this investigation. One common method of using twitter is to mark messages with hashtags, which makes them viewable and searchable to the entire twitter audience. This strategy is especially used when users are commenting on an external event (e.g. a sporting, media, or political event). We focus here on the World Series of baseball, an annual sporting event with large viewership; in this case, the hashtag is \texttt{\#worldseries}. Hashtagged messages thus form a case in which there is \emph{no} prior linguistic context. The total shared context with the audience that can be assumed by the writer of a tweet is the non-linguistic content of the event being hashtagged. 
%
%We begin by describing our corpus and our method of calculating linguistic content (by computing entropy within a simple $n$-gram model). We then investigate gradual changes in word-by-word information content as the event goes on (testing Equation \ref{eq:gc}) and rapid changes in the total information content of tweets in response to important in-game events (testing Equation \ref{eq:prop}). We end by considering a number of control analyses that provide evidence against alternative accounts of our results. 

\section{Corpus and Methods}

\subsection{\#worldseries Corpus}

Our current analysis looked at tweets during the 2014 World Series, a series of seven baseball games in late October 2014.  We obtained these tweets by searching publicly-available tweets through the Twitter API, using an adaptation of SeeTweet \cite{doyle2014} to compile tweets containing the hashtag ``\#WorldSeries.''  To synchronize tweets with game events, we used the Major League Baseball Advance Media XML repository,\footnote{\url{http://gd2.mlb.com/components/game/mlb/}} which contains pitch-by-pitch data including the ongoing state of the game and timestamps at the start of each at-bat. Using this timestamp information, we bin tweets by at-bats so that they can be effectively co-registered with other in-game statistics.  These bins extend to the beginning of the next at-bat, and thus provide time for reactions to the events of the at-bat.\footnote{We tested a series of potential offset times in case Twitter and MLB used different clocks or in case at-bats were not long enough to capture reactions. We did not adjust the times as there was no significant increase in the correlation between Leverage Index (Sect. \ref{sect:other-metrics}) and tweet rate for these offsets.}  The mean at-bat length was 2.76 minutes, and there were 512 total at-bats.  We limited our analysis to tweets timestamped during one of these at-bats, resulting in a total corpus of 109,207 tweets. Each game had its first pitch at approximately 0008 UTC, and lasted between three and four hours.

These tweets were compiled from the ``garden-hose'' Twitter search API, which returns a subset of all relevant tweets. In practice, our searches captured approximately 4\% of all relevant tweets; Twitter reported 420,329 relevant tweets during Game 1 of the World Series\footnote{\url{https://twitter.com/TwitterData/status/524972545930301440}}, and our dataset contained 17,538 tweets during the same time period.  We address potential confounds from this sampling in Section \ref{sect:speaker-normalization}.

\subsection{Entropy Computation}

We estimated the linguistic information content of each tweet in our corpus. Social media text has been described as ``bad language'' \cite{eisenstein2013}, and can be difficult to model due to its idiosyncratic abbreviations, typographic errors, and other non-standard forms. Relevant to our goal of assessing information content, it can be difficult to create an appropriate training corpus for language models, since vocabulary and composition of tweets of change substantially on both across and within days \cite{eisenstein2013}.

We attempted to minimize these difficulties in two ways.  First, we used tweet length (in characters) as our initial metric of information content. Unless information rate varies systematically across tweets of different lengths---counter to the claim of approximately uniform information density in language\cite{genzel2002,levy2007}---longer tweets will generally carry more information.

Second, we estimated language models with domain-specific corpora. In particular, for tweets from each game we used a training corpus consisting of the tweets from all the other games. This training set provided a vocabulary and structure that was similar in topic and style and to the test set.  We removed all punctuation and emoji except word-initial {\it @} and {\it \#}, which refer to users and hashtags, respectively.  Usernames were replaced with {\it [MENTION]} to reduce sparsity; hashtags were not altered, as these often function as words or phrases within the tweet's syntax.  Words with fewer than 5 occurrences in the training corpus were marked as out-of-vocabulary items. We estimated trigram models using a modification of NLTK \cite{bird2006}\footnote{Smoothing on n-gram models in NLTK can be inaccurate (see \url{https://github.com/nltk/nltk/issues/367}), so we used a modified version from Brandon Roy.} with Witten-Bell smoothing, and estimated per-word and total entropy for each tweet from these models.

%\begin{equation}
%H_L = \sum_i{p ~ log(p)} - FIXME
%\end{equation}

\section{Gradual Changes in Information Rate}

Our first analytic goal was to examine changes in the information content of tweets due to the long-term build-up of context in a shared event.  We predicted that we would see similar developments in information structure as in more traditional conversational settings, even though there was no formal conversation or explicit linguistic history to develop common ground.  Specifically, we predicted that the build-up of contextual information would cause the context-independent per-word entropy to rise over time, replicating the effect that has been observed across languages and genres\cite{genzel2002,genzel2003,qian2012}.

\begin{figure}
 \centering
  \includegraphics[width=3.25in]{figures/time-perword-ent-agg}
 \caption{Per-word entropy increases with time for the first two hours of the games, then levels off and slightly declines. Loess curve fitting with 95\% confidence intervals.}\label{fig:time-perword-ent}\vspace*{-.5em}
\end{figure}

\begin{table*}
  \begin{tabular}{clc}
 Minute & Tweet & Per-word entropy \\
\hline
0 & \#WorldSeries Play Ball & 4.96\\
0 & It's finally here! \#WorldSeries & 4.74\\
0 & \parbox[][6ex][c]{.7\textwidth}{IDEA: @mayoredlee, \#SanFrancisco can pledge to throw our @SFGiants an \#OrangeOctober parade regardless of \#WorldSeries outcome! \#SFGiants} & 8.20\\
\hline
12 & The guy with the Marlins sweater is behind home plate again. \#worldseries & 4.26\\
12 & \parbox[][6ex][c]{.7\textwidth}{Something about Hunter Pence really, really bothers me. Don't ask me what, cause I havent figured it out, but I don't like him. \#WorldSeries} & 6.64\\
12 & The Giants 3-0! \#WorldSeries & 5.43\\
\hline
73 & \parbox[][6ex][c]{.7\textwidth}{Three HORRIBLE at-bats (mixed in with Cain's walk) prevent Royals from breaking through in the third. \#WorldSeries} & 9.39\\
130 & \parbox[][6ex][c]{.7\textwidth}{As Hardy Boy \#2, Joe Panik just pulled the mask off of Vargas and discovered it's Old Man Withers from down the street. \#WorldSeries} & 8.12\\
178 & \parbox[][6ex][c]{.7\textwidth}{\#WorldSeries it's funny the non body names have a great hits. Frm now n on consider the Postseson as Cinderla run.  No names needed, \#MLB} & 10.04\\
\hline
  \end{tabular}
 \caption{Example tweets, grouped by minutes since the first pitch.}\label{tab:ex}
\end{table*}

Figure \ref{fig:time-perword-ent} shows evidence for changes in per-word entropy over the course of games. Per-word entropy rises sharply in the first half-hour of each game, then begins to level off and finally declines slightly over time.  This pattern is consistent with the constant entropy rate proposal of \cite{genzel2002}, and more specifically with the context decay model of \cite{qian2012}.\footnote{A late decline in per-word entropy also appeared in \citealt{qian2012}'s analysis of Swedish.} 

We use a mixed-effects linear regression to test this relationship, using the time of an at-bat to predict both per-word and per-tweet entropy during the at-bat. Specifically, we use the logarithm of time, per the context-decay models of \cite{qian2012}.  We add game-specific random intercepts and slopes of log-time to capture cross-game variation. Significant positive effects of time on entropy are found through likelihood-ratio tests for both models (per-word: $.348 \pm .045; p<.001, \chi^2(3)=104.6$, per-tweet: $10.31 \pm 2.08; p=.001, \chi^2(3)=74.65$). %Significant positive effects of time on entropy are found through likelihood-ratio tests for both models (per-word: $.080 \pm .016; p<.001, \chi^2(1)=11.02$, per-tweet: $2.61 \pm .74; p=.008, \chi^2(1)=7.12$).

A later tweet with the same in-context entropy as an earlier tweet will have a higher entropy when estimated out-of-context. We hypothesize that this finding is due to the accrual of common ground across users from shared non-linguistic information. As they watch more of the game, they share more referents and have stronger expectations about what aspects of the game will be discussed. This shared common ground licenses more complex language and more sophisticated linguistic references. Table \ref{tab:ex} gives example tweets at different time points; as a game progresses, references can expand from generic references to the teams or series, to specific individuals and events, and eventually to sequences of events.

While this finding is consistent with previous work on the effect of context \cite{genzel2002,qian2012}, it expands the definition of context.  In previous work, the context came from explicit linguistic information, as it tested written paragraphs.  In the Twitter dataset, the context comes from real-world events during the games, as there is no canonical shared sequence of tweets that the tweeters can refer back to.  Contextual influences on entropy need not be explicitly linguistic, so long as discourse participants have reason to believe that the other participants share their knowledge.

\section{Fast Changes In Information Content}

\begin{figure}
 \centering
  \includegraphics[width=3.25in]{figures/rate-total-ent-agg}
 \caption{Total tweet entropy plotted against log tweet rate. Color reflects in-game time; line shows the best linear fit with 95\% confidence intervals.}\label{fig:time-perword-ent}\vspace*{-.5em}
\end{figure}

\begin{table*}
  \begin{tabular}{clc}
Log rate & Tweet & Per-word entropy \\
%\hline
%2.61 & Butler channeled his inner "The Little Engine That Could" running the bases! \#Royals \#Game7 \#WorldSeries & 6.30\\
%2.61 & Was \#Butler Running Is Slo-Mo!!! \#WorldSeries \#Game7 & 8.16\\
%2.61 & Aside from chasing after a food truck, I don't think I've ever seen Billy Butler run that fast! Hang tough, \#KC! \#WorldSeries \#Royals & 7.70\\
\hline
2.49 & Fuck you, Blanco. \#Giants \#WorldSeries & 5.54\\
2.49 & Holy shitballs, @Royals! \#WorldSeries \#Game7 & 3.99\\
2.49 & Just when you thought the \#WorldSeries was over.... \#E8 & 4.76\\
\hline
1.66 & \parbox[][6ex][c]{.7\textwidth}{I suppose I appreciate Bochy's ``ASG'' approach with Bumgarner. Of course, who are any of us to question him in late October? \#WorldSeries} & 7.42\\[3pt]
1.66 & \parbox[][6ex][c]{.7\textwidth}{The guy in Marlins gear behind home plate needs to escorted off property for annoying everybody. \#WorldSeries \#WhoDoesThat} & 4.85\\[3pt]
1.66 & Lets Go Giants!!! 5-0  \#SFGiants \#WorldSeries & 3.26\\
\hline
  \end{tabular}
 \caption{Example tweets, grouped by the per-minute tweet rate during each at-bat.}\label{tab:ex2}
\end{table*}

Our second analytic goal was to examine fast changes in information content in response to transient in-game events. By Equation \ref{eq:joint-entropy}, unexpected events should have large information content and the linguistic information content should be reduced to maintain constant entropy. Intuitively, after an exciting, game-changing event, tweets will be shorter and make more reference to the shared knowledge that this event has just happened. Such events should also generate more responses, suggesting that the number of tweets per unit time can serve as a proxy for the information content of an event.

Table \ref{tab:ex2} shows suggestive evidence of this, with examples of consecutive tweets from high-rate and low-rate at-bats, along with their information content. The top triplet comes from one of the highest-rate at-bats, in which Gregor Blanco committed a crucial error in the last inning of the last game, which these tweets all refer to.  The bottom triplet comes from a low-rate at-bat, mid-game in a developing blowout, and the tweets all refer to different things as there is no salient shared event.

%This short-timescale adaptation is predicted for two reasons. First, it represents a rational response to issues of information overload, the state where the amount of incoming information exceeds a user's ability to process it \cite{miller1956}.  Previous investigations into online forum posting behavior have shown such rational responses on a longer timescale \cite{jones2001a,jones2001b,whittaker2003,schoberth2003}, as well as the more explicitly conversational setting of IRC chat channels \cite{jones2008}.  Second, given that the hashtagged tweets are tied to an ongoing real-world event, changes in tweet rates are likely to be tied to what is happening in the event, providing a potential proxy for the non-linguistic context available at the time of the tweet. We will expand on this second explanation in Section \ref{sect:other-metrics}.

We tested this prediction by fitting a mixed-effect linear regression model using the logarithm of per-minute tweet rate as a predictor of tweet entropy.  Given its significance in the previous model, we included log(time) as a control factor in this analysis, and added by-game random intercepts and slopes for log(rate) and log(time).  The log of the tweet rate had a significant negative effect on per-word and per-tweet entropy by likelihood-ratio tests (per-word: $-.333 \pm .073; p<.001, \chi^2(4)=59.37$, per-tweet: $-21.82 \pm 2.43; p<.001, \chi^2(4)=194.6$). 

Log(time) retained significance ($p<.001$) as a predictor for both entropy measures even when rate was accounted for, showing evidence for both slow and fast adaptation occurring in the discourse.  The effects are also pointed in the predicted directions.  Entropy increases with time as more informative context builds up. Entropy decreases with tweet rate, suggesting that more exciting events encourage less information-laden tweets.

\section{Control Analyses}

\subsection{Non-Rate Metrics of Context}\label{sect:other-metrics}
Tweet rate is an important metric for examining fast adaptation, but it could be conflated with other factors influencing tweet production.  For instance, there is evidence that online interactions exhibit rational responses to potential information overload, the state where the amount of incoming information exceeds a user's ability to process it \cite{miller1956,schoberth2003}.  Previous investigations into forum posting behavior have shown adaptations to potential overload \cite{jones2001a,jones2001b,whittaker2003,schoberth2003}, and a similar result was found for the more explicitly conversational setting of IRC chat channels \cite{jones2008}.   

To show that the changes in information content are not merely reactions to increased tweet competition but have independent informational motivations, we need metrics of event importance and predictability that are not dependent on social media behavior.  Baseball has a long history of statistical analysis, and as a result, there are independently-derived metrics that fit this bill.  Two that are appropriate for this purpose are {\it Leverage Index} (LI)\footnote{\url{http://www.hardballtimes.com/crucial-situations/}} and  {\it Win Probability Added} (WPA) \cite{tango2006}.

LI is an estimate of how critical an at-bat is to the outcome of the game.  It is based on the difference in resultant win probability if the current batter gets a hit or an out, normalized by the mean change in win probability over all at-bats. 1 is the average LI, and greater LI indicates greater importance.  LI, as a measure of the expected change in win probability, is similar to non-linguistic entropy term in Equation \ref{eq:joint-entropy}.  WPA depends on the result of an at-bat, and estimates how much the win probability changed as a result of what happened during the at-bat.  WPA thus provides an estimate of how much information about the game outcome this at-bat has provided, conditioned on the current game context.  These measures are well-correlated (Kendall's $\tau=.77$), since a high-LI at-bat's value comes from its ability to affect win probability.

As high LI or WPA values indicate an at-bat whose result has a large effect on the game, these metrics provide an estimate for non-linguistic informativity that is independent of medium-specific influences on tweet production.  To assess their effects, we constructed four mixed-effects linear regression models, using either LI or WPA to predict either per-word or per-tweet entropy.  We built separate models including LI and including WPA due to their high correlation. Fixed- and by-game random-effects of log(time) and log(rate) were included as controls in all models; if there is an effect of LI or WPA beyond the effect of rate, this is evidence of fast speaker adaptation to non-linguistic information content.

Both LI and WPA had significant negative effects on per-tweet entropy (LI: $-1.52 \pm .43; p=.001, \chi^2(5)=20.1$, WPA: $-2.27 \pm .40; p<.001, \chi^2(5)=44.18$), over and above the effect of tweet rate.  Per-word entropy did not show a significant effect of LI or WPA when rate was included as a control factor. Each was a significant factor on per-word entropy ($p=.008,p=.005$) when rate was not included as a control, though, suggestive that the explanatory power of these independent metrics may be subsumed in the more complex factor of tweet rate.

\subsection{Speaker Normalization}

An alternative hypothesis for the observed behavioral changes with tweet rate is that they arise not from changes in the behavior of individuals but rather from a change in demographics.  It is plausible that rising tweet rates come from an influx of new tweeters into the hashtag, and that these new tweeters simply produce shorter, less informative tweets in general.  For instance, spambots often include trending hashtags in their spam tweets \cite{martinez2013}. To account for this, we treated the users whose tweets are in our corpus as a ``computational focus group'' \cite{lin2013,lin2014}, and used the Twitter API to collected a further 100 tweets from each user outside the timeframe of the games.  We used these tweets to estimate an average tweet length for each user, and subtracted this value from the length of their \#worldseries tweets during the games.  If this baselined metric displays the same effects as shown above, we have reason to believe that users are in fact changing their individual behaviors in response to information factors, rather than that a demographic shift is mimicking a behavioral shift.

First, we created a mixed-effects model with WPA, log(rate) and log(time) as predictors of tweet length.  All three factors were significant (WPA: $-1.64 \pm .36; p<.001, \chi^2(5)=72.3$; log(rate): $-6.15 \pm .47; p<.001, \chi^2(5) =303.6 $; log(time): $.82 \pm .40; p=.001, \chi^2(5)=20.6$).  We then created a second model using the same factors to predict the mean change in tweet length from the baseline length. Again, all three factors were significant (WPA: $-2.01 \pm .29; p<.001, \chi^2(5)=70.2$; log(rate): $-5.10 \pm .49; p<.001, \chi^2(5) = 252.6$; log(time): $.61 \pm .35; p=.016, \chi^2(5)=14.0$).  This suggests that tweeters do shift their behavior in resposne to these factors, rather than them resulting form a change in who is tweeting.

\section{Discussion}


Summary

Non-linguistic content

Internet as a source of non-linguist context

Causal bottleneck theory of surprisal. \cite{hale2001,levy2008} If there is a surprising event going on, processing difficulty should track with the summed difficulty. 

Conclusion

% In a G&C-type setting, all of the context is coming from linguistic information, and that certainly makes sense in a written setting; where else could context come from? In the course of reading, the reader pays an integration cost with each word, which is proportional to the KL divergence from the distribution over upcoming words before and after reading the current word.  That KL-divergence cost works out to just be the surprisal of the word, and the expected surprisal is exactly what entropy is.  So the constant H(X_i|C_i,L_i) term in G&C's equation is that integration cost -- that's why it's constant, because it's the channel capacity.

% All right, what does that mean when we have both linguistic and non-linguistic context to integrate? Well, our integration capacity remains constant, but now it's allotted to the joint information content H(T_j,E_j|C_{j-1}). T_j is the linguistic content in the j-th timestep, E_j is the new event content in the j-th timestep, and C_{j-1} is all the ling & non-ling context amassed prior to the j-th timestep. 

\section*{Acknowledgments}

We gratefully acknowledge the support of ONR Grant N00014-13-1-0287.

\bibliographystyle{naaclhlt2015}
\bibliography{tweetprag}

\end{document}
